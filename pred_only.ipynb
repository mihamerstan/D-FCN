{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/michael/python-virtual-environments/pointscnn/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:469: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/michael/python-virtual-environments/pointscnn/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:470: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/michael/python-virtual-environments/pointscnn/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:471: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/michael/python-virtual-environments/pointscnn/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:472: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/michael/python-virtual-environments/pointscnn/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:473: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/michael/python-virtual-environments/pointscnn/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:476: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/usr/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/michael/lidar/D-FCN\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import math\n",
    "from datetime import datetime\n",
    "#import h5pyprovider\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import socket\n",
    "import importlib\n",
    "import matplotlib.pylab as plt\n",
    "import os\n",
    "import pickle\n",
    "# os.environ['CUDA_DEVICE_ORDER']=\"PCI_BUS_ID\"\n",
    "# os.environ['CUDA_VISIBLE_DEVICES']=\"1\"\n",
    "\n",
    "import sys\n",
    "BASE_DIR = os.path.abspath('')\n",
    "print(BASE_DIR)\n",
    "sys.path.append(os.path.join(BASE_DIR, 'models'))\n",
    "sys.path.append(BASE_DIR) # model\n",
    "sys.path.append(os.path.join(BASE_DIR, 'tf_utils'))\n",
    "import provider\n",
    "import tf_util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Argparses\n",
    "EPOCH_CNT = 0\n",
    "\n",
    "BATCH_SIZE = 6\n",
    "NUM_POINT = 2048\n",
    "MAX_EPOCH = 200\n",
    "BASE_LEARNING_RATE = .01\n",
    "# GPU_INDEX = 7\n",
    "GPU_INDEX = 1\n",
    "MOMENTUM = 0.9\n",
    "OPTIMIZER = 'adam'\n",
    "DECAY_STEP = 20000\n",
    "DECAY_RATE = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASE_DIR /home/michael/lidar/D-FCN/tf_utils\n"
     ]
    }
   ],
   "source": [
    "MODEL = importlib.import_module('DFCN_pointnet2_group2') # import network module\n",
    "MODEL_FILE = os.path.join(BASE_DIR, 'DFCN_pointnet2_group2'+'.py')\n",
    "LOG_DIR = 'log_wen_v16_sample8192_group2_lw14_F1_noheight'\n",
    "if not os.path.exists(LOG_DIR): os.mkdir(LOG_DIR)\n",
    "os.system('cp models/DFCN_pointnet2.py %s' % (LOG_DIR)) # bkp of model def\n",
    "os.system('cp tf_utils/DFCN_util_xy2.py %s' % (LOG_DIR)) # bkp of model def\n",
    "# os.system('cp train_pointsift_lx_npsplit_V16.ipynb %s' % (LOG_DIR)) # bkp of train procedure\n",
    "LOG_FOUT = open(os.path.join(LOG_DIR, 'log_train.txt'), 'w')\n",
    "# LOG_FOUT.write(str(FLAGS)+'\\n')\n",
    "\n",
    "BN_INIT_DECAY = 0.5\n",
    "BN_DECAY_DECAY_RATE = 0.5\n",
    "BN_DECAY_DECAY_STEP = float(DECAY_STEP)\n",
    "BN_DECAY_CLIP = 0.99\n",
    "\n",
    "HOSTNAME = socket.gethostname()\n",
    "\n",
    "NUM_CLASSES = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_string(out_str):\n",
    "    LOG_FOUT.write(out_str+'\\n')\n",
    "    LOG_FOUT.flush()\n",
    "    print(out_str)\n",
    "\n",
    "def get_learning_rate(batch):\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "                        BASE_LEARNING_RATE,  # Base learning rate.\n",
    "                        batch * BATCH_SIZE,  # Current index into the dataset.\n",
    "                        DECAY_STEP,          # Decay step.\n",
    "                        DECAY_RATE,          # Decay rate.\n",
    "                        staircase=True)\n",
    "    learning_rate = tf.maximum(learning_rate, 0.00001) # CLIP THE LEARNING RATE!!\n",
    "    return learning_rate        \n",
    "\n",
    "def get_bn_decay(batch):\n",
    "    bn_momentum = tf.train.exponential_decay(\n",
    "                      BN_INIT_DECAY,\n",
    "                      batch*BATCH_SIZE,\n",
    "                      BN_DECAY_DECAY_STEP,\n",
    "                      BN_DECAY_DECAY_RATE,\n",
    "                      staircase=True)\n",
    "    bn_decay = tf.minimum(BN_DECAY_CLIP, 1 - bn_momentum)\n",
    "    return bn_decay\n",
    "\n",
    "def Acc_from_confusions(confusions):\n",
    "    \n",
    "    TP = np.diagonal(confusions, axis1=-2, axis2=-1)\n",
    "    TP_plus_FN = np.sum(confusions, axis=-1)\n",
    "    TP_plus_FP = np.sum(confusions, axis=-2)\n",
    "    \n",
    "    mAcc = np.sum(TP)/np.sum(confusions)\n",
    "    \n",
    "    precision = TP / (TP_plus_FP + 1e-6)\n",
    "    recall = TP / (TP_plus_FN+ 1e-6)\n",
    "    fscore = 2*(precision * recall)/(precision + recall + 1e-6)\n",
    "    \n",
    "    ave_F1 = np.mean(fscore)\n",
    "    \n",
    "    s = 'Overall accuracy：{:5.2f}  Average F1 score：{:5.2f} \\n'.format(100 * mAcc, 100 * ave_F1)\n",
    "    s += log_acc(precision)\n",
    "    s += log_acc(recall)\n",
    "    s += log_acc(fscore)\n",
    "    \n",
    "    log_string(s)\n",
    "    \n",
    "    return mAcc, ave_F1\n",
    "    \n",
    "\n",
    "def log_acc(acc_list):\n",
    "    s = \"\"\n",
    "    for acc in acc_list:\n",
    "        s += '{:5.2f} '.format(100 * acc)\n",
    "    s += '\\n'\n",
    "    return s\n",
    "\n",
    "def pc_normalize(pc):\n",
    "    l = pc.shape[0]\n",
    "    centroid = np.mean(pc, axis=0)\n",
    "    pc = pc - centroid\n",
    "    m = np.max(np.sqrt(np.sum(pc**2, axis=1)))\n",
    "    pc = pc / m\n",
    "    return pc\n",
    "    \n",
    "def drawPlot(x,y,name):\n",
    "    plt.rcParams['savefig.dpi'] = 300 \n",
    "    plt.plot(np.arange(0,len(x)),x,'k-',alpha=1,label='Train max: '+str(round(max(x),3))+', min: '+str(round(min(x),3)))\n",
    "    plt.plot(np.arange(0,len(y)),y,'r-',alpha=1,label='Test max: '+str(round(max(y),3))+', min: '+str(round(min(y),3)))\n",
    "    plt.legend()\n",
    "    plt.xlabel('epoch',fontsize=9)\n",
    "    plt.ylabel(name+' value',fontsize=9)\n",
    "    plt.savefig(LOG_DIR+\"/\"+name+\".png\",bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "def drawF1Plot(x,name):\n",
    "    plt.rcParams['savefig.dpi'] = 300 \n",
    "    plt.plot(np.arange(0,len(x)),x,'k-',alpha=1,label='Train max: '+str(round(max(x),3))+', min: '+str(round(min(x),3)))\n",
    "    plt.legend()\n",
    "    plt.xlabel('epoch',fontsize=9)\n",
    "    plt.ylabel(name+' value',fontsize=9)\n",
    "    plt.savefig(LOG_DIR+\"/\"+name+\".png\",bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def placeholder_inputs(batch_size, num_point):\n",
    "    pointclouds_pl = tf.placeholder(tf.float32, shape=(None, None, 3))\n",
    "    labels_pl = tf.placeholder(tf.int32, shape=(None, None))\n",
    "    smpws_pl = tf.placeholder(tf.float32, shape=(None, None))\n",
    "    return pointclouds_pl, labels_pl, smpws_pl\n",
    "\n",
    "def pc_normalize_min_max(data):\n",
    "    mindata = np.min(data[:,:3], axis=0)\n",
    "    maxdata = np.max(data[:,:3], axis=0)\n",
    "    return 2*(data[:,:3] - mindata)/(maxdata - mindata)\n",
    "\n",
    "def pc_normalize_min(data):\n",
    "    mindata = np.min(data[:,:3], axis=0)\n",
    "    \n",
    "    return (data[:,:3] - mindata)\n",
    "\n",
    "def get_batch(dataset, index, npoints = NUM_POINT):\n",
    "  \n",
    "    if(dataset =='train'):\n",
    "        cub_l = 30.0\n",
    "        cub_w = 30.0\n",
    "        cub_h = 100.0\n",
    "        point_set =  trainSet[:,:3] - np.min(trainSet[:,:3], axis=0)\n",
    "        semantic_seg = trainSet[:,4].astype(np.int32)\n",
    "        coordmax = np.max(point_set,axis=0)\n",
    "        coordmin = np.min(point_set,axis=0)\n",
    "        smpmin = np.maximum(coordmax-[cub_l,cub_w,cub_h], coordmin)\n",
    "        smpmin[2] = coordmin[2]\n",
    "        smpsz = np.minimum(coordmax-smpmin,[cub_l,cub_w,cub_h])\n",
    "        smpsz[2] = coordmax[2]-coordmin[2]\n",
    "        isvalid = False\n",
    "        for i in range(10):\n",
    "            curcenter = point_set[np.random.choice(len(semantic_seg),1)[0],:]\n",
    "            curmin = curcenter-[cub_l/2,cub_w/2,cub_h/2]\n",
    "            curmax = curcenter+[cub_l/2,cub_w/2,cub_h/2]\n",
    "            curmin[2] = coordmin[2]\n",
    "            curmax[2] = coordmax[2]\n",
    "            curchoice = np.sum((point_set>=(curmin-0.0))*(point_set<(curmax+0.0)),axis=1)==3\n",
    "            cur_point_set = point_set[curchoice,:]\n",
    "            cur_semantic_seg = semantic_seg[curchoice]\n",
    "            cur_feat_set = trainFeats[curchoice,:]\n",
    "    #         if len(cur_semantic_seg)<npoints:\n",
    "            if len(cur_semantic_seg)==0:\n",
    "                continue\n",
    "            mask = np.sum((cur_point_set>=(curmin-0.0))*(cur_point_set<(curmax+0.0)),axis=1)==3\n",
    "            vidx = np.ceil((cur_point_set[mask,:2]-curmin[:2])/(curmax[:2]-curmin[:2])*[31.0,31.0])\n",
    "            vidx = np.unique(vidx[:,0]*31.0+vidx[:,1])\n",
    "            isvalid = np.sum(cur_semantic_seg>-1)/1.0/len(cur_semantic_seg)>=0.7 and len(vidx)/31.0/31.0>=0.3\n",
    "#             print('isvalid', isvalid,len(vidx)/31.0/31.0,np.sum(cur_semantic_seg>-1),len(cur_semantic_seg))\n",
    "            if isvalid:\n",
    "                break\n",
    "        choice = np.random.choice(len(cur_semantic_seg), npoints, replace=True)\n",
    "        point_set = cur_point_set[choice,:]\n",
    "        feature_set = cur_feat_set[choice,:]\n",
    "        semantic_seg = cur_semantic_seg[choice]\n",
    "        mask = mask[choice]\n",
    "        sample_weight = labelweights[semantic_seg]\n",
    "        sample_weight *= mask\n",
    "        return point_set, semantic_seg, sample_weight,feature_set\n",
    "    \n",
    "    if(dataset =='test'):\n",
    "\n",
    "        cur_point_set = test_xyz[index]\n",
    "        cur_semantic_seg = test_label[index].astype(np.int32)\n",
    "        feature_set = test_feats[index]\n",
    "\n",
    "        point_set = pc_normalize_min(cur_point_set)\n",
    "        semantic_seg = cur_semantic_seg # N\n",
    "        sample_weight = labelweights_t[semantic_seg]\n",
    "    \n",
    "        point_sets = np.expand_dims(point_set,0) # 1xNx3\n",
    "        feature_set = np.expand_dims(feature_set,0) # 1xNx3\n",
    "        semantic_segs = np.expand_dims(semantic_seg,0)  # 1xN\n",
    "        sample_weights = np.expand_dims(sample_weight,0)  # 1xN\n",
    "        return point_sets, semantic_segs, sample_weights,feature_set\n",
    "    \n",
    "def get_batch_wdp(dataset, batch_idx):\n",
    "    bsize = BATCH_SIZE\n",
    "    batch_data = np.zeros((bsize, NUM_POINT, 3))\n",
    "    batch_feats = np.zeros((bsize, NUM_POINT, 1))\n",
    "    batch_label = np.zeros((bsize, NUM_POINT), dtype=np.int32)\n",
    "    batch_smpw = np.zeros((bsize, NUM_POINT), dtype=np.float32)\n",
    "    for i in range(bsize):\n",
    "        ps,seg,smpw,feat = get_batch('train',index=0)\n",
    "        ps = pc_normalize_min(ps)\n",
    "        batch_data[i,...] = ps\n",
    "        batch_label[i,:] = seg\n",
    "        batch_smpw[i,:] = smpw\n",
    "        batch_feats[i,:] = feat\n",
    "\n",
    "        dropout_ratio = np.random.random()*0.875 # 0-0.875\n",
    "        drop_idx = np.where(np.random.random((ps.shape[0]))<=dropout_ratio)[0]\n",
    "        batch_data[i,drop_idx,:] = batch_data[i,0,:]\n",
    "        batch_label[i,drop_idx] = batch_label[i,0]\n",
    "        batch_smpw[i,drop_idx] *= 0\n",
    "        \n",
    "    return batch_data, batch_label, batch_smpw, batch_feats   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.967452  2.0217078 1.9802364 2.9339767 2.874856  2.1228907 2.7627099\n",
      " 2.6275027 2.1882594]\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# train_f = open('Data/train_merge_min_norm_fea.pickle', 'rb')\n",
    "# train_xyz, train_label, train_feats = pickle.load(train_f, encoding='bytes')\n",
    "# train_xyz, train_label, train_feats = pickle.load(train_f)\n",
    "# train_f.close()\n",
    "\n",
    "test_f = open('Data/test_merge_min_norm_fea_paper_height.pickle', 'rb')\n",
    "test_xyz, test_label, test_feats = pickle.load(test_f, encoding='bytes')\n",
    "test_feats = [tt[:,1:2] for tt in test_feats] #reflectance\n",
    "test_f.close()\n",
    "\n",
    "NUM_CLASSES = 9\n",
    "label_values = range(NUM_CLASSES)\n",
    "\n",
    "trainSet = np.loadtxt('Data/train_height.pts',skiprows=1)\n",
    "\n",
    "label_w = trainSet[:,4].astype('uint8')\n",
    "trainSet[:,3] = trainSet[:,3]/trainSet[:,3].max() #height above ground\n",
    "trainSet[:,5] = trainSet[:,5]/trainSet[:,5].max() #reflectance\n",
    "\n",
    "# trainFeats = trainSet[:,[3,5]] #use reflectance and height above ground\n",
    "trainFeats = trainSet[:,5:6] #only use reflectance\n",
    "\n",
    "labelweights = np.zeros(9)\n",
    "tmp,_ = np.histogram(label_w,range(10))\n",
    "labelweights = tmp\n",
    "labelweights = labelweights.astype(np.float32)\n",
    "labelweights = labelweights/np.sum(labelweights)\n",
    "labelweights = 1/np.log(1.4+labelweights)\n",
    "print(labelweights)\n",
    "\n",
    "labelweights_t = np.ones(9)\n",
    "print(labelweights_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(sess, ops, train_writer):\n",
    "    \"\"\" ops: dict mapping from string to tf ops \"\"\"\n",
    "    is_training = True\n",
    "    \n",
    "    log_string('----')\n",
    "    \n",
    "   # Shuffle train samples\n",
    "    train_idxs = np.arange(0, len(train_xyz))\n",
    "    np.random.shuffle(train_idxs)\n",
    "    num_batches = len(train_xyz)//BATCH_SIZE\n",
    "    \n",
    "    total_correct = 0\n",
    "    total_seen = 0\n",
    "    loss_sum = 0\n",
    "    \n",
    "    for batch_idx in range(num_batches):\n",
    "        \n",
    "        start_idx = batch_idx * BATCH_SIZE\n",
    "        end_idx = (batch_idx+1) * BATCH_SIZE\n",
    "        \n",
    "        batch_data, batch_label, batch_smpw, batch_feats = get_batch_wdp('train', batch_idx)\n",
    "        \n",
    "        if batch_idx % (num_batches/2) == 0:\n",
    "            print('Current batch/total batch num: %d/%d'%(batch_idx,num_batches))\n",
    "        \n",
    "        aug_data = provider.rotate_point_cloud_z(batch_data)\n",
    "        \n",
    "        feed_dict = {ops['pointclouds_pl']: aug_data,\n",
    "                     ops['feature_pl']: batch_feats,\n",
    "                     ops['labels_pl']: batch_label,\n",
    "                     ops['smpws_pl']: batch_smpw,\n",
    "                     ops['is_training_pl']: is_training,}\n",
    "        summary, step, _, loss_val, pred_val, lr_val = sess.run([ops['merged'], ops['step'], ops['train_op'], ops['loss'], ops['pred'], ops['learnrate']],\n",
    "                                         feed_dict=feed_dict)\n",
    "        train_writer.add_summary(summary, step)\n",
    "        pred_val = np.argmax(pred_val, 2)\n",
    "        correct = np.sum(pred_val == batch_label)\n",
    "        total_correct += correct\n",
    "        total_seen += (BATCH_SIZE*NUM_POINT)\n",
    "        loss_sum += loss_val\n",
    "        \n",
    "    log_string('learn rate: %f' % (lr_val))\n",
    "    log_string('mean loss: %f' % (loss_sum / float(num_batches)))\n",
    "    log_string('accuracy: %f' % (total_correct / float(total_seen)))\n",
    "    \n",
    "    mloss = loss_sum / float(num_batches)\n",
    "    macc = total_correct / float(total_seen)\n",
    "    return mloss, macc\n",
    "\n",
    "def eval_one_epoch_whole_scene(sess, ops):\n",
    "    \"\"\" ops: dict mapping from string to tf ops \"\"\"\n",
    "    is_training = False\n",
    "    total_correct = 0\n",
    "    total_seen = 0\n",
    "    loss_sum = 0\n",
    "    total_seen_class = [0 for _ in range(NUM_CLASSES)]\n",
    "    total_correct_class = [0 for _ in range(NUM_CLASSES)]\n",
    "    \n",
    "    log_string('----')\n",
    "    \n",
    "    test_idxs = np.arange(0, len(test_xyz))\n",
    "    \n",
    "    TEST_BATCH_SIZE = 1\n",
    "    num_batches = len(test_xyz)\n",
    "    \n",
    "    Confs = []\n",
    "    \n",
    "    \n",
    "    is_continue_batch = False\n",
    "    \n",
    "    for batch_idx in range(num_batches):\n",
    "        \n",
    "        batch_data, batch_label, batch_smpw, batch_feats = get_batch('test', batch_idx)\n",
    "        \n",
    "#         print('Current start end /total batch num: %d %d/%d'%(start_idx, end_idx, num_batches))\n",
    "        \n",
    "        aug_data = batch_data\n",
    "        \n",
    "#         aug_data = provider.rotate_point_cloud_z(batch_data)\n",
    "        \n",
    "        feed_dict = {ops['pointclouds_pl']: aug_data,\n",
    "                     ops['feature_pl']: batch_feats,\n",
    "                     ops['labels_pl']: batch_label,\n",
    "                     ops['smpws_pl']: batch_smpw,\n",
    "                     ops['is_training_pl']: is_training}\n",
    "        print(\"feed_dict: \",feed_dict)\n",
    "        summary, step, loss_val, pred_val, lr_val = sess.run([ops['merged'], ops['step'], ops['loss'], ops['pred'], ops['learnrate']],\n",
    "                                      feed_dict=feed_dict)\n",
    "        \n",
    "        pred_val = np.argmax(pred_val, 2)\n",
    "        correct = np.sum(pred_val == batch_label)\n",
    "        total_correct += correct\n",
    "        total_seen += batch_data.shape[1]\n",
    "        loss_sum += loss_val\n",
    "        \n",
    "        NUM_POINT_fact = batch_data.shape[1]\n",
    "        for i in range(TEST_BATCH_SIZE):\n",
    "            for j in range(NUM_POINT_fact):\n",
    "                l = batch_label[i, j]\n",
    "                total_seen_class[l] += 1\n",
    "                total_correct_class[l] += (pred_val[i, j] == l)\n",
    "                \n",
    "        from sklearn.metrics import confusion_matrix\n",
    "        Confs += [confusion_matrix(batch_label.flatten(), pred_val.flatten(), label_values)]\n",
    "        \n",
    "    C = np.sum(np.stack(Confs), axis=0).astype(np.float32)\n",
    "    oa, avgF1 = Acc_from_confusions(C)\n",
    "    \n",
    "    log_string('learn rate: %f' % (lr_val))\n",
    "    log_string('eval mean loss: %f' % (loss_sum / float(num_batches)))\n",
    "    log_string('eval accuracy: %f'% (total_correct / float(total_seen)))\n",
    "    log_string('eval avg class acc: %f' % (np.mean(np.array(total_correct_class)/np.array(total_seen_class,dtype=np.float))))\n",
    "    \n",
    "    mloss = loss_sum / float(num_batches)\n",
    "    macc = total_correct / float(total_seen)\n",
    "    return oa, avgF1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pointclouds_pl, labels_pl, smpws_pl = placeholder_inputs(BATCH_SIZE, NUM_POINT)\n",
    "feature_pl = tf.placeholder(tf.float32, shape=(None, None, 1))\n",
    "is_training_pl = tf.placeholder(tf.bool, shape=())\n",
    "\n",
    "# Note the global_step=batch parameter to minimize. \n",
    "# That tells the optimizer to helpfully increment the 'batch' parameter for you every time it trains.\n",
    "batch = tf.Variable(0)\n",
    "bn_decay = get_bn_decay(batch)\n",
    "tf.summary.scalar('bn_decay', bn_decay)\n",
    "\n",
    "# Get model and loss \n",
    "pred, end_points = MODEL.get_model(pointclouds_pl, is_training_pl, NUM_CLASSES, bn_decay=bn_decay, feature=feature_pl)\n",
    "loss = MODEL.get_loss(pred, labels_pl, smpws_pl)\n",
    "\n",
    "tf.summary.scalar('loss', loss)\n",
    "\n",
    "correct = tf.equal(tf.argmax(pred, 2), tf.to_int64(labels_pl))\n",
    "accuracy = tf.reduce_sum(tf.cast(correct, tf.float32)) / float(BATCH_SIZE*NUM_POINT)\n",
    "tf.summary.scalar('accuracy', accuracy)\n",
    "\n",
    "# Get training operator\n",
    "learning_rate = get_learning_rate(batch)\n",
    "tf.summary.scalar('learning_rate', learning_rate)\n",
    "if OPTIMIZER == 'momentum':\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate, momentum=MOMENTUM)\n",
    "elif OPTIMIZER == 'adam':\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "train_op = optimizer.minimize(loss, global_step=batch)\n",
    "\n",
    "# Add ops to save and restore all the variables.\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sess' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-d4d113373294>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Init variables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0minit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mis_training_pl\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m ops = {'pointclouds_pl': pointclouds_pl,\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sess' is not defined"
     ]
    }
   ],
   "source": [
    "# Add summary writers\n",
    "merged = tf.summary.merge_all()\n",
    "\n",
    "# Init variables\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init, {is_training_pl:True})\n",
    "\n",
    "ops = {'pointclouds_pl': pointclouds_pl,\n",
    "       'labels_pl': labels_pl,\n",
    "       'feature_pl': feature_pl,\n",
    "       'smpws_pl': smpws_pl,\n",
    "       'is_training_pl': is_training_pl,\n",
    "       'pred': pred,\n",
    "       'loss': loss,\n",
    "       'train_op': train_op,\n",
    "       'merged': merged,\n",
    "       'step': batch,\n",
    "       'learnrate': learning_rate}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ops' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-fcb86227d6bc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'ops' is not defined"
     ]
    }
   ],
   "source": [
    "ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from log_wen_v16_sample8192_group2_lw14_F1_noheight/best_model_cleared.ckpt\n",
      "----\n",
      "feed_dict:  {<tf.Tensor 'Placeholder:0' shape=(?, ?, 3) dtype=float32>: array([[[ 1.59375   ,  9.5       ,  0.13000488],\n",
      "        [ 1.59375   ,  9.5       ,  0.10998535],\n",
      "        [ 1.59375   ,  9.5       ,  0.13998413],\n",
      "        ...,\n",
      "        [29.9375    ,  4.        , 12.32000732],\n",
      "        [29.9375    ,  4.        , 12.38000488],\n",
      "        [29.90625   ,  3.5       , 12.51000977]]]), <tf.Tensor 'Placeholder_3:0' shape=(?, ?, 1) dtype=float32>: array([[[0.047059],\n",
      "        [0.058824],\n",
      "        [0.05098 ],\n",
      "        ...,\n",
      "        [0.003922],\n",
      "        [0.070588],\n",
      "        [0.058824]]]), <tf.Tensor 'Placeholder_1:0' shape=(?, ?) dtype=int32>: array([[2, 2, 2, ..., 5, 5, 5]], dtype=int32), <tf.Tensor 'Placeholder_2:0' shape=(?, ?) dtype=float32>: array([[1., 1., 1., ..., 1., 1., 1.]]), <tf.Tensor 'Placeholder_4:0' shape=() dtype=bool>: False}\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Cannot interpret feed_dict key as Tensor: Tensor Tensor(\"Placeholder:0\", shape=(?, ?, 3), dtype=float32) is not an element of this graph.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m~/python-virtual-environments/pointscnn/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1063\u001b[0m             subfeed_t = self.graph.as_graph_element(subfeed, allow_tensor=True,\n\u001b[0;32m-> 1064\u001b[0;31m                                                     allow_operation=False)\n\u001b[0m\u001b[1;32m   1065\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/python-virtual-environments/pointscnn/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mas_graph_element\u001b[0;34m(self, obj, allow_tensor, allow_operation)\u001b[0m\n\u001b[1;32m   3034\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3035\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_as_graph_element_locked\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_operation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3036\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/python-virtual-environments/pointscnn/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_as_graph_element_locked\u001b[0;34m(self, obj, allow_tensor, allow_operation)\u001b[0m\n\u001b[1;32m   3113\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3114\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Tensor %s is not an element of this graph.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3115\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Tensor Tensor(\"Placeholder:0\", shape=(?, ?, 3), dtype=float32) is not an element of this graph.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-9a7ff12c3aee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0msaver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_meta_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLOG_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'best_model_cleared.ckpt.meta'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLOG_DIR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0meval_one_epoch_whole_scene\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-5e72619b86c2>\u001b[0m in \u001b[0;36meval_one_epoch_whole_scene\u001b[0;34m(sess, ops)\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"feed_dict: \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         summary, step, loss_val, pred_val, lr_val = sess.run([ops['merged'], ops['step'], ops['loss'], ops['pred'], ops['learnrate']],\n\u001b[0;32m---> 88\u001b[0;31m                                       feed_dict=feed_dict)\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0mpred_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/python-virtual-environments/pointscnn/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/python-virtual-environments/pointscnn/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1065\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1066\u001b[0m             raise TypeError('Cannot interpret feed_dict key as Tensor: '\n\u001b[0;32m-> 1067\u001b[0;31m                             + e.args[0])\n\u001b[0m\u001b[1;32m   1068\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1069\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubfeed_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Cannot interpret feed_dict key as Tensor: Tensor Tensor(\"Placeholder:0\", shape=(?, ?, 3), dtype=float32) is not an element of this graph."
     ]
    }
   ],
   "source": [
    "\n",
    "# Add ops to save and restore all the variables.\n",
    "LOG_DIR = 'log_wen_v16_sample8192_group2_lw14_F1_noheight'\n",
    "epoch = 65\n",
    "ops = {'pointclouds_pl': pointclouds_pl,\n",
    "       'labels_pl': labels_pl,\n",
    "       'feature_pl': feature_pl,\n",
    "       'smpws_pl': smpws_pl,\n",
    "       'is_training_pl': is_training_pl,\n",
    "       'pred': pred,\n",
    "       'loss': loss,\n",
    "       'train_op': train_op,\n",
    "       'merged': merged,\n",
    "       'step': batch,\n",
    "       'learnrate': learning_rate}\n",
    "# tf.reset_default_graph()\n",
    "# saver = tf.train.Saver()\n",
    "\n",
    "# # Create a session\n",
    "# config = tf.ConfigProto()\n",
    "# config.gpu_options.allow_growth = True\n",
    "# config.allow_soft_placement = True\n",
    "# config.log_device_placement = True\n",
    "# sess = tf.Session(config=config)\n",
    "tf.reset_default_graph()\n",
    "# K.clear_session()\n",
    "with tf.Session() as sess:\n",
    "    saver = tf.train.import_meta_graph(os.path.join(LOG_DIR, 'best_model_cleared.ckpt.meta'))\n",
    "    saver.restore(sess, tf.train.latest_checkpoint(LOG_DIR))\n",
    "    eval_one_epoch_whole_scene(sess, ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
