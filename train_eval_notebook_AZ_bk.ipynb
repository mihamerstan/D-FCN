{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/az/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "/home/az/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/az/anaconda3/lib/python3.6/site-packages/tensorflow/include\n",
      "/home/az/anaconda3/lib/python3.6/site-packages/tensorflow\n"
     ]
    }
   ],
   "source": [
    "    import tensorflow as tf\n",
    "    # include path\n",
    "    print(tf.sysconfig.get_include())\n",
    "    # library path \n",
    "    print(tf.sysconfig.get_lib())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Hello, TensorFlow!'\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "CUDA_VISIBLE_DEVICES=0\n",
    "hello = tf.constant('Hello, TensorFlow!')\n",
    "sess = tf.Session()\n",
    "print(sess.run(hello))\n",
    "#python3 -c 'import tensorflow as tf; print(tf.test.is_gpu_available())'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "/device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "print(tf.test.is_gpu_available())\n",
    "print(tf.test.gpu_device_name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[name: \"/device:CPU:0\"\n",
       " device_type: \"CPU\"\n",
       " memory_limit: 268435456\n",
       " locality {\n",
       " }\n",
       " incarnation: 8203134522506823368, name: \"/device:GPU:0\"\n",
       " device_type: \"GPU\"\n",
       " memory_limit: 155582464\n",
       " locality {\n",
       "   bus_id: 1\n",
       " }\n",
       " incarnation: 17780709368865929169\n",
       " physical_device_desc: \"device: 0, name: GeForce RTX 2060 SUPER, pci bus id: 0000:01:00.0, compute capability: 7.5\"]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.is_built_with_cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/az/Documents/D-FCN\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import math\n",
    "from datetime import datetime\n",
    "#import h5pyprovider\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import socket\n",
    "import importlib\n",
    "import matplotlib.pylab as plt\n",
    "import os\n",
    "import pickle\n",
    "# os.environ['CUDA_DEVICE_ORDER']=\"PCI_BUS_ID\"\n",
    "# os.environ['CUDA_VISIBLE_DEVICES']=\"1\"\n",
    "\n",
    "import sys\n",
    "BASE_DIR = os.path.abspath('')\n",
    "print(BASE_DIR)\n",
    "sys.path.append(os.path.join(BASE_DIR, 'models'))\n",
    "sys.path.append(BASE_DIR) # model\n",
    "sys.path.append(os.path.join(BASE_DIR, 'tf_utils'))\n",
    "import provider\n",
    "import tf_util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Argparses\n",
    "EPOCH_CNT = 0\n",
    "\n",
    "BATCH_SIZE = 6\n",
    "NUM_POINT = 2048\n",
    "MAX_EPOCH = 200\n",
    "BASE_LEARNING_RATE = .01\n",
    "# GPU_INDEX = 7\n",
    "GPU_INDEX = 1\n",
    "MOMENTUM = 0.9\n",
    "OPTIMIZER = 'adam'\n",
    "DECAY_STEP = 20000\n",
    "DECAY_RATE = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASE_DIR /home/az/Documents/D-FCN/tf_utils\n"
     ]
    }
   ],
   "source": [
    "MODEL = importlib.import_module('DFCN_pointnet2_group2') # import network module\n",
    "MODEL_FILE = os.path.join(BASE_DIR, 'DFCN_pointnet2_group2'+'.py')\n",
    "LOG_DIR = 'log_wen_v16_sample8192_group2_lw14_F1_noheight'\n",
    "if not os.path.exists(LOG_DIR): os.mkdir(LOG_DIR)\n",
    "os.system('cp models/DFCN_pointnet2.py %s' % (LOG_DIR)) # bkp of model def\n",
    "os.system('cp tf_utils/DFCN_util_xy2.py %s' % (LOG_DIR)) # bkp of model def\n",
    "# os.system('cp train_pointsift_lx_npsplit_V16.ipynb %s' % (LOG_DIR)) # bkp of train procedure\n",
    "LOG_FOUT = open(os.path.join(LOG_DIR, 'log_train.txt'), 'w')\n",
    "# LOG_FOUT.write(str(FLAGS)+'\\n')\n",
    "\n",
    "BN_INIT_DECAY = 0.5\n",
    "BN_DECAY_DECAY_RATE = 0.5\n",
    "BN_DECAY_DECAY_STEP = float(DECAY_STEP)\n",
    "BN_DECAY_CLIP = 0.99\n",
    "\n",
    "HOSTNAME = socket.gethostname()\n",
    "\n",
    "NUM_CLASSES = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_string(out_str):\n",
    "    LOG_FOUT.write(out_str+'\\n')\n",
    "    LOG_FOUT.flush()\n",
    "    print(out_str)\n",
    "\n",
    "def get_learning_rate(batch):\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "                        BASE_LEARNING_RATE,  # Base learning rate.\n",
    "                        batch * BATCH_SIZE,  # Current index into the dataset.\n",
    "                        DECAY_STEP,          # Decay step.\n",
    "                        DECAY_RATE,          # Decay rate.\n",
    "                        staircase=True)\n",
    "    learning_rate = tf.maximum(learning_rate, 0.00001) # CLIP THE LEARNING RATE!!\n",
    "    return learning_rate        \n",
    "\n",
    "def get_bn_decay(batch):\n",
    "    bn_momentum = tf.train.exponential_decay(\n",
    "                      BN_INIT_DECAY,\n",
    "                      batch*BATCH_SIZE,\n",
    "                      BN_DECAY_DECAY_STEP,\n",
    "                      BN_DECAY_DECAY_RATE,\n",
    "                      staircase=True)\n",
    "    bn_decay = tf.minimum(BN_DECAY_CLIP, 1 - bn_momentum)\n",
    "    return bn_decay\n",
    "\n",
    "def Acc_from_confusions(confusions):\n",
    "    \n",
    "    TP = np.diagonal(confusions, axis1=-2, axis2=-1)\n",
    "    TP_plus_FN = np.sum(confusions, axis=-1)\n",
    "    TP_plus_FP = np.sum(confusions, axis=-2)\n",
    "    \n",
    "    mAcc = np.sum(TP)/np.sum(confusions)\n",
    "    \n",
    "    precision = TP / (TP_plus_FP + 1e-6)\n",
    "    recall = TP / (TP_plus_FN+ 1e-6)\n",
    "    fscore = 2*(precision * recall)/(precision + recall + 1e-6)\n",
    "    \n",
    "    ave_F1 = np.mean(fscore)\n",
    "    \n",
    "    s = 'Overall accuracy：{:5.2f}  Average F1 score：{:5.2f} \\n'.format(100 * mAcc, 100 * ave_F1)\n",
    "    s += log_acc(precision)\n",
    "    s += log_acc(recall)\n",
    "    s += log_acc(fscore)\n",
    "    \n",
    "    log_string(s)\n",
    "    \n",
    "    return mAcc, ave_F1\n",
    "    \n",
    "\n",
    "def log_acc(acc_list):\n",
    "    s = \"\"\n",
    "    for acc in acc_list:\n",
    "        s += '{:5.2f} '.format(100 * acc)\n",
    "    s += '\\n'\n",
    "    return s\n",
    "\n",
    "def pc_normalize(pc):\n",
    "    l = pc.shape[0]\n",
    "    centroid = np.mean(pc, axis=0)\n",
    "    pc = pc - centroid\n",
    "    m = np.max(np.sqrt(np.sum(pc**2, axis=1)))\n",
    "    pc = pc / m\n",
    "    return pc\n",
    "    \n",
    "def drawPlot(x,y,name):\n",
    "    plt.rcParams['savefig.dpi'] = 300 \n",
    "    plt.plot(np.arange(0,len(x)),x,'k-',alpha=1,label='Train max: '+str(round(max(x),3))+', min: '+str(round(min(x),3)))\n",
    "    plt.plot(np.arange(0,len(y)),y,'r-',alpha=1,label='Test max: '+str(round(max(y),3))+', min: '+str(round(min(y),3)))\n",
    "    plt.legend()\n",
    "    plt.xlabel('epoch',fontsize=9)\n",
    "    plt.ylabel(name+' value',fontsize=9)\n",
    "    plt.savefig(LOG_DIR+\"/\"+name+\".png\",bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "def drawF1Plot(x,name):\n",
    "    plt.rcParams['savefig.dpi'] = 300 \n",
    "    plt.plot(np.arange(0,len(x)),x,'k-',alpha=1,label='Train max: '+str(round(max(x),3))+', min: '+str(round(min(x),3)))\n",
    "    plt.legend()\n",
    "    plt.xlabel('epoch',fontsize=9)\n",
    "    plt.ylabel(name+' value',fontsize=9)\n",
    "    plt.savefig(LOG_DIR+\"/\"+name+\".png\",bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def placeholder_inputs(batch_size, num_point):\n",
    "    pointclouds_pl = tf.placeholder(tf.float32, shape=(None, None, 3))\n",
    "    labels_pl = tf.placeholder(tf.int32, shape=(None, None))\n",
    "    smpws_pl = tf.placeholder(tf.float32, shape=(None, None))\n",
    "    return pointclouds_pl, labels_pl, smpws_pl\n",
    "\n",
    "def pc_normalize_min_max(data):\n",
    "    mindata = np.min(data[:,:3], axis=0)\n",
    "    maxdata = np.max(data[:,:3], axis=0)\n",
    "    return 2*(data[:,:3] - mindata)/(maxdata - mindata)\n",
    "\n",
    "def pc_normalize_min(data):\n",
    "    mindata = np.min(data[:,:3], axis=0)\n",
    "    # MS: Function now returns mindata here\n",
    "    return (data[:,:3] - mindata), mindata\n",
    "\n",
    "\n",
    "def get_batch(dataset, index, npoints = NUM_POINT):\n",
    "  \n",
    "    if(dataset =='train'):\n",
    "        cub_l = 30000000.0\n",
    "        cub_w = 30000000.0\n",
    "        cub_h = 100000000.0\n",
    "        point_set =  trainSet[:,:3] - np.min(trainSet[:,:3], axis=0)\n",
    "        semantic_seg = trainSet[:,4].astype(np.int32)\n",
    "        coordmax = np.max(point_set,axis=0)\n",
    "        coordmin = np.min(point_set,axis=0)\n",
    "        smpmin = np.maximum(coordmax-[cub_l,cub_w,cub_h], coordmin)\n",
    "        smpmin[2] = coordmin[2]\n",
    "        smpsz = np.minimum(coordmax-smpmin,[cub_l,cub_w,cub_h])\n",
    "        smpsz[2] = coordmax[2]-coordmin[2]\n",
    "        isvalid = False\n",
    "        for i in range(10):\n",
    "            curcenter = point_set[np.random.choice(len(semantic_seg),1)[0],:]\n",
    "            curmin = curcenter-[cub_l/2,cub_w/2,cub_h/2]\n",
    "            curmax = curcenter+[cub_l/2,cub_w/2,cub_h/2]\n",
    "            curmin[2] = coordmin[2]\n",
    "            curmax[2] = coordmax[2]\n",
    "            curchoice = np.sum((point_set>=(curmin-0.0))*(point_set<(curmax+0.0)),axis=1)==3\n",
    "            cur_point_set = point_set[curchoice,:]\n",
    "            cur_semantic_seg = semantic_seg[curchoice]\n",
    "            cur_feat_set = trainFeats[curchoice,:]\n",
    "    #         if len(cur_semantic_seg)<npoints:\n",
    "            if len(cur_semantic_seg)==0:\n",
    "                continue\n",
    "            mask = np.sum((cur_point_set>=(curmin-0.0))*(cur_point_set<(curmax+0.0)),axis=1)==3\n",
    "            vidx = np.ceil((cur_point_set[mask,:2]-curmin[:2])/(curmax[:2]-curmin[:2])*[31.0,31.0])\n",
    "            vidx = np.unique(vidx[:,0]*31.0+vidx[:,1])\n",
    "            isvalid = np.sum(cur_semantic_seg>-1)/1.0/len(cur_semantic_seg)>=0.7 and len(vidx)/31.0/31.0>=0.3\n",
    "#             print('isvalid', isvalid,len(vidx)/31.0/31.0,np.sum(cur_semantic_seg>-1),len(cur_semantic_seg))\n",
    "            if isvalid:\n",
    "                break\n",
    "        choice = np.random.choice(len(cur_semantic_seg), npoints, replace=True)\n",
    "        point_set = cur_point_set[choice,:]\n",
    "        feature_set = cur_feat_set[choice,:]\n",
    "        semantic_seg = cur_semantic_seg[choice]\n",
    "        mask = mask[choice]\n",
    "        sample_weight = labelweights[semantic_seg]\n",
    "        sample_weight *= mask\n",
    "        return point_set, semantic_seg, sample_weight,feature_set\n",
    "    \n",
    "    if(dataset =='test'):\n",
    "        ##original 30/30/100\n",
    "        #100 - 1000 = 500\n",
    "        #300000\n",
    "        #1000000\n",
    "        cub_l = 30000000.0\n",
    "        cub_w = 30000000.0\n",
    "        cub_h = 100000000.0\n",
    "        #all rows, 0&1&2 columns = xyz \n",
    "        point_set =  testSet[:,:3] - np.min(testSet[:,:3], axis=0)#minimum in column?/ min of x,y,z\n",
    "        #5th column = classification, used for batch label for corretness\n",
    "        semantic_seg = testSet[:,4].astype(np.int32)\n",
    "        coordmax = np.max(point_set,axis=0)\n",
    "        coordmin = np.min(point_set,axis=0)\n",
    "        smpmin = np.maximum(coordmax-[cub_l,cub_w,cub_h], coordmin) #cub covers much more pts in our dataset\n",
    "        smpmin[2] = coordmin[2]#y?\n",
    "        smpsz = np.minimum(coordmax-smpmin,[cub_l,cub_w,cub_h])\n",
    "        smpsz[2] = coordmax[2]-coordmin[2]\n",
    "        isvalid = False\n",
    "        for i in range(10):\n",
    "            curcenter = point_set[np.random.choice(len(semantic_seg),1)[0],:]\n",
    "            curmin = curcenter-[cub_l/2,cub_w/2,cub_h/2]\n",
    "            curmax = curcenter+[cub_l/2,cub_w/2,cub_h/2] \n",
    "            curmin[2] = coordmin[2]\n",
    "            curmax[2] = coordmax[2]\n",
    "            curchoice = np.sum((point_set>=(curmin-0.0))*(point_set<(curmax+0.0)),axis=1)==3\n",
    "            cur_point_set = point_set[curchoice,:]#!\n",
    "            cur_semantic_seg = semantic_seg[curchoice]#\n",
    "            cur_feat_set = testFeats[curchoice,:]\n",
    "    #         if len(cur_semantic_seg)<npoints:\n",
    "            if len(cur_semantic_seg)==0:\n",
    "                continue\n",
    "            mask = np.sum((cur_point_set>=(curmin-0.0))*(cur_point_set<(curmax+0.0)),axis=1)==3\n",
    "            vidx = np.ceil((cur_point_set[mask,:2]-curmin[:2])/(curmax[:2]-curmin[:2])*[31.0,31.0])\n",
    "            vidx = np.unique(vidx[:,0]*31.0+vidx[:,1])\n",
    "            isvalid = np.sum(cur_semantic_seg>-1)/1.0/len(cur_semantic_seg)>=0.7 and len(vidx)/31.0/31.0>=0.3\n",
    "#             print('isvalid', isvalid,len(vidx)/31.0/31.0,np.sum(cur_semantic_seg>-1),len(cur_semantic_seg))\n",
    "            if isvalid:\n",
    "                break\n",
    "            \n",
    "        choice = np.random.choice(len(cur_semantic_seg), npoints, replace=True) #npoints= 2048 = repeat\n",
    "        point_set = cur_point_set[choice,:]#! repeat #choose x,y,z, n rows \n",
    "        feature_set = cur_feat_set[choice,:]\n",
    "        semantic_seg = cur_semantic_seg[choice]\n",
    "        mask = mask[choice]\n",
    "        sample_weight = labelweights[semantic_seg]\n",
    "        sample_weight *= mask\n",
    "        return point_sets, semantic_segs, sample_weights,feature_set\n",
    "    \n",
    "def get_batch_wdp(dataset, batch_idx):\n",
    "    bsize = BATCH_SIZE\n",
    "    batch_data = np.zeros((bsize, NUM_POINT, 3))\n",
    "    ps_xyz_min = np.zeros((bsize, 1, 3))\n",
    "    ps_out = np.zeros((bsize, NUM_POINT, 3))\n",
    "    \n",
    "    batch_feats = np.zeros((bsize, NUM_POINT, 1))\n",
    "    batch_label = np.zeros((bsize, NUM_POINT), dtype=np.int32)\n",
    "    batch_smpw = np.zeros((bsize, NUM_POINT), dtype=np.float32)\n",
    "    for i in range(bsize):\n",
    "        ps,seg,smpw,feat = get_batch('train',index=0)\n",
    "        #ps_out[i,...] = ps\n",
    "        ps,ps_min = pc_normalize_min(ps)\n",
    "        batch_data[i,...] = ps\n",
    "        batch_label[i,:] = seg\n",
    "        batch_smpw[i,:] = smpw\n",
    "        batch_feats[i,:] = feat\n",
    "\n",
    "        dropout_ratio = np.random.random()*0.875 # 0-0.875\n",
    "        drop_idx = np.where(np.random.random((ps.shape[0]))<=dropout_ratio)[0]\n",
    "        batch_data[i,drop_idx,:] = batch_data[i,0,:]\n",
    "        batch_label[i,drop_idx] = batch_label[i,0]\n",
    "        batch_smpw[i,drop_idx] *= 0\n",
    "        #ps_xyz_min[i,...] = ps_min\n",
    "        \n",
    "    return batch_data, batch_label, batch_smpw, batch_feats\n",
    "\n",
    "def get_batch_wdp_test(dataset, batch_idx):\n",
    "    bsize = BATCH_SIZE\n",
    "    batch_data = np.zeros((bsize, NUM_POINT, 3))\n",
    "    ps_xyz_min = np.zeros((bsize, 1, 3))\n",
    "    ps_out = np.zeros((bsize, NUM_POINT, 3))\n",
    "\n",
    "    batch_feats = np.zeros((bsize, NUM_POINT, 1))\n",
    "    batch_label = np.zeros((bsize, NUM_POINT), dtype=np.int32)\n",
    "    batch_smpw = np.zeros((bsize, NUM_POINT), dtype=np.float32)\n",
    "    for i in range(bsize):#6\n",
    "        #point_set, semantic_seg, sample_weight,feature_set,cur_semantic_seg#cur_semantic_seg\n",
    "        ps,seg,smpw,feat,cur_semantic_seg = get_batch('test',index=0)#\n",
    "        # ps_out collects the pre-normalized batch_data\n",
    "        ps_out[i,...] = ps\n",
    "        # ps_min is collected not from pc_normalize_min\n",
    "        ps,ps_min = pc_normalize_min(ps)\n",
    "        batch_data[i,...] = ps\n",
    "        batch_label[i,:] = seg\n",
    "        batch_smpw[i,:] = smpw\n",
    "        batch_feats[i,:] = feat\n",
    "\n",
    "        dropout_ratio = np.random.random()*0.875 # 0-0.875\n",
    "        drop_idx = np.where(np.random.random((ps.shape[0]))<=dropout_ratio)[0]\n",
    "        batch_data[i,drop_idx,:] = batch_data[i,0,:]\n",
    "        batch_label[i,drop_idx] = batch_label[i,0]\n",
    "        batch_smpw[i,drop_idx] *= 0\n",
    "    \t# ps_xyz_min collects the xyz minimums vector for each batch\n",
    "        ps_xyz_min[i,...] = ps_min\n",
    "    # ps_xyz_min and px_out are now returned.\n",
    "    #cur_semantic_seg\n",
    "    return batch_data, batch_label, batch_smpw, batch_feats, ps_xyz_min, ps_out,cur_semantic_seg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.1561419 2.9720137 2.9720137 2.8228025 2.9720137 2.9720137 2.9720137\n",
      " 2.9720137 2.9720137]\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/az/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:23: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# train_f = open('Data/train_merge_min_norm_fea.pickle', 'rb')\n",
    "# train_xyz, train_label, train_feats = pickle.load(train_f, encoding='bytes')\n",
    "# train_xyz, train_label, train_feats = pickle.load(train_f)\n",
    "# train_f.close()\n",
    "\n",
    "testSet = np.loadtxt('Data/03labeled_seg.pts',skiprows=1)\n",
    "label_w = testSet[:,4].astype('uint8')\n",
    "if (testSet[:,3].max() != 0):\n",
    "    testSet[:,3] = testSet[:,3]/testSet[:,3].max() #height above ground\n",
    "if (testSet[:,5].max() != 0):\n",
    "    testSet[:,5] = testSet[:,5]/testSet[:,5].max() #reflectance\n",
    "    \n",
    "testFeats = testSet[:,5:6] #only use reflectance\n",
    "\n",
    "NUM_CLASSES = 9\n",
    "label_values = range(NUM_CLASSES)\n",
    "\n",
    "#trainSet = np.loadtxt('Data/train_bk.pts',skiprows=1)\n",
    "trainSet = np.loadtxt('Data/03labeled_seg.pts',skiprows=1)\n",
    "\n",
    "label_w = trainSet[:,4].astype('uint8')\n",
    "trainSet[:,3] = trainSet[:,3]/trainSet[:,3].max() #height above ground\n",
    "trainSet[:,5] = trainSet[:,5]/trainSet[:,5].max() #reflectance\n",
    "\n",
    "# trainFeats = trainSet[:,[3,5]] #use reflectance and height above ground\n",
    "trainFeats = trainSet[:,5:6] #only use reflectance\n",
    "\n",
    "labelweights = np.zeros(9)\n",
    "tmp,_ = np.histogram(label_w,range(10))\n",
    "labelweights = tmp\n",
    "labelweights = labelweights.astype(np.float32)\n",
    "labelweights = labelweights/np.sum(labelweights)\n",
    "labelweights = 1/np.log(1.4+labelweights)\n",
    "print(labelweights)\n",
    "\n",
    "labelweights_t = np.ones(9)\n",
    "print(labelweights_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(sess, ops, train_writer):\n",
    "    \"\"\" ops: dict mapping from string to tf ops \"\"\"\n",
    "    is_training = True\n",
    "    \n",
    "    log_string('----')\n",
    "    \n",
    "    total_correct = 0\n",
    "    total_seen = 0\n",
    "    loss_sum = 0\n",
    "    \n",
    "    \n",
    "   # Shuffle train samples\n",
    "    train_idxs = np.arange(0, len(train_xyz))\n",
    "    np.random.shuffle(train_idxs)\n",
    "    num_batches = len(train_xyz)//BATCH_SIZE\n",
    "    \n",
    "    for batch_idx in range(num_batches):\n",
    "        \n",
    "        start_idx = batch_idx * BATCH_SIZE\n",
    "        end_idx = (batch_idx+1) * BATCH_SIZE\n",
    "        \n",
    "        batch_data, batch_label, batch_smpw, batch_feats = get_batch_wdp('train', batch_idx)\n",
    "        \n",
    "        if batch_idx % (num_batches/2) == 0:\n",
    "            print('Current batch/total batch num: %d/%d'%(batch_idx,num_batches))\n",
    "        \n",
    "        aug_data = provider.rotate_point_cloud_z(batch_data)\n",
    "        \n",
    "        feed_dict = {ops['pointclouds_pl']: aug_data,\n",
    "                     ops['feature_pl']: batch_feats,\n",
    "                     ops['labels_pl']: batch_label,\n",
    "                     ops['smpws_pl']: batch_smpw,\n",
    "                     ops['is_training_pl']: is_training,}\n",
    "        summary, step, _, loss_val, pred_val, lr_val = sess.run([ops['merged'], ops['step'], ops['train_op'], \n",
    "                                                                 ops['loss'], ops['pred'], ops['learnrate']],\n",
    "                                         feed_dict=feed_dict)\n",
    "        #summary, step, _, loss_val, pred_val, lr_val = sess.run([ops['merged'], ops['step'], ops['train_op'], \n",
    "        #                                                         ops['loss'], ops['pred'], ops['learnrate']],\n",
    "                                         #                        feed_dict=feed_dict)\n",
    "        train_writer.add_summary(summary, step)\n",
    "        pred_val = np.argmax(pred_val, 2)\n",
    "        correct = np.sum(pred_val == batch_label)\n",
    "        total_correct += correct\n",
    "        total_seen += (BATCH_SIZE*NUM_POINT)\n",
    "        loss_sum += loss_val\n",
    "        \n",
    "    log_string('learn rate: %f' % (lr_val))\n",
    "    log_string('mean loss: %f' % (loss_sum / float(num_batches)))\n",
    "    log_string('accuracy: %f' % (total_correct / float(total_seen)))\n",
    "    \n",
    "    mloss = loss_sum / float(num_batches)\n",
    "    macc = total_correct / float(total_seen)\n",
    "    return mloss, macc\n",
    "\n",
    "def eval_one_epoch_whole_scene(sess, ops, test_writer):\n",
    "    \"\"\" ops: dict mapping from string to tf ops \"\"\"\n",
    "    is_training = False\n",
    "    \n",
    "    total_correct = 0\n",
    "    total_seen = 0\n",
    "    loss_sum = 0\n",
    "    total_seen_class = [0 for _ in range(NUM_CLASSES)]\n",
    "    total_correct_class = [0 for _ in range(NUM_CLASSES)]\n",
    "    \n",
    "    log_string('----')\n",
    "    \n",
    "    test_idxs = np.arange(0, len(test_xyz))\n",
    "    # Shuffle train samples\n",
    "    np.random.shuffle(test_idxs)\n",
    "    num_batches = len(test_xyz)//BATCH_SIZE\n",
    "    \n",
    "    #TEST_BATCH_SIZE = 1\n",
    "    #num_batches = len(test_xyz)\n",
    "    \n",
    "    Confs = []\n",
    "    \n",
    "    #\n",
    "    #Confs_normalized = []\n",
    "    #pred_list = []\n",
    "    \n",
    "    is_continue_batch = False\n",
    "    \n",
    "    for batch_idx in range(num_batches):\n",
    "        \n",
    "        start_idx = batch_idx * BATCH_SIZE\n",
    "        end_idx = (batch_idx+1) * BATCH_SIZE\n",
    "        #cur_semantic_seg\n",
    "        #batch_data, batch_label, batch_smpw, batch_feats, ps_xyz_min, ps_out,cur_semantic_seg = get_batch_wdp_test('test', batch_idx)\n",
    "        batch_data, batch_label, batch_smpw, batch_feats = get_batch_wdp_test('test', batch_idx)\n",
    "        if batch_idx % (num_batches/2) == 0:\n",
    "            print('Current batch/total batch num: %d/%d'%(batch_idx,num_batches))\n",
    "\n",
    "            \n",
    "        aug_data, rotation_matrix = provider.rotate_point_cloud_z(batch_data)\n",
    "        \n",
    "        feed_dict = {ops['pointclouds_pl']: aug_data,\n",
    "                     ops['feature_pl']: batch_feats,\n",
    "                     ops['labels_pl']: batch_label,\n",
    "                     ops['smpws_pl']: batch_smpw,\n",
    "                     ops['is_training_pl']: is_training}\n",
    "        print(\"feed_dict: \",feed_dict)\n",
    "\n",
    "        summary, step, _, loss_val, pred_val, lr_val = sess.run([ops['merged'], ops['step'], ops['train_op'], ops['loss'], ops['pred'], ops['learnrate']],\n",
    "                                         feed_dict=feed_dict)\n",
    "        \n",
    "        pred_val = np.argmax(pred_val, 2)\n",
    "        correct = np.sum(pred_val == batch_label)\n",
    "        #pred_list.append([ps_out, pred_val])\n",
    "        \n",
    "        total_correct += correct\n",
    "        total_seen += (BATCH_SIZE*NUM_POINT)\n",
    "        loss_sum += loss_val\n",
    "        \n",
    "       \n",
    "        #NUM_POINT_fact = batch_data.shape[1]\n",
    "        NUM_POINT_fact = (BATCH_SIZE*NUM_POINT)\n",
    "        for i in range(BATCH_SIZE):\n",
    "            for j in range(NUM_POINT):\n",
    "                l = batch_label[i, j]\n",
    "                total_seen_class[l] += 1\n",
    "                total_correct_class[l] += (pred_val[i, j] == l)\n",
    "                \n",
    "        from sklearn.metrics import confusion_matrix\n",
    "        Confs += [confusion_matrix(batch_label.flatten(), pred_val.flatten(), label_values)]\n",
    "        #add normalize = true\n",
    "        #x/total true viechles\n",
    "        Confs_normalized += [confusion_matrix(batch_label.flatten(), pred_val.flatten(), label_values, normalize='true')]\n",
    "    \n",
    "    #end of for loop\n",
    "    C = np.sum(np.stack(Confs), axis=0).astype(np.float32)\n",
    "    #C_normalized = np.sum(np.stack(Confs_normalized), axis=0).astype(np.float32)\n",
    "    oa, avgF1 = Acc_from_confusions(C)\n",
    "   \n",
    "    log_string('learn rate: %f' % (lr_val))\n",
    "    log_string('eval mean loss: %f' % (loss_sum / float(num_batches)))\n",
    "    log_string('eval accuracy: %f'% (total_correct / float(total_seen)))\n",
    "    log_string('eval avg class acc: %f' % (np.mean(np.array(total_correct_class)/np.array(total_seen_class,dtype=np.float))))\n",
    "    \n",
    "    mloss = loss_sum / float(num_batches)\n",
    "    macc = total_correct / float(total_seen)\n",
    "    return oa, avgF1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pointclouds_pl, labels_pl, smpws_pl = placeholder_inputs(BATCH_SIZE, NUM_POINT)\n",
    "feature_pl = tf.placeholder(tf.float32, shape=(None, None, 1))\n",
    "is_training_pl = tf.placeholder(tf.bool, shape=())\n",
    "\n",
    "# Note the global_step=batch parameter to minimize. \n",
    "# That tells the optimizer to helpfully increment the 'batch' parameter for you every time it trains.\n",
    "batch = tf.Variable(0)\n",
    "bn_decay = get_bn_decay(batch)\n",
    "tf.summary.scalar('bn_decay', bn_decay)\n",
    "\n",
    "# Get model and loss \n",
    "pred, end_points = MODEL.get_model(pointclouds_pl, is_training_pl, NUM_CLASSES, bn_decay=bn_decay, feature=feature_pl)\n",
    "loss = MODEL.get_loss(pred, labels_pl, smpws_pl)\n",
    "\n",
    "tf.summary.scalar('loss', loss)\n",
    "\n",
    "correct = tf.equal(tf.argmax(pred, 2), tf.to_int64(labels_pl))\n",
    "accuracy = tf.reduce_sum(tf.cast(correct, tf.float32)) / float(BATCH_SIZE*NUM_POINT)\n",
    "tf.summary.scalar('accuracy', accuracy)\n",
    "\n",
    "# Get training operator\n",
    "learning_rate = get_learning_rate(batch)\n",
    "tf.summary.scalar('learning_rate', learning_rate)\n",
    "if OPTIMIZER == 'momentum':\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate, momentum=MOMENTUM)\n",
    "elif OPTIMIZER == 'adam':\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "train_op = optimizer.minimize(loss, global_step=batch)\n",
    "\n",
    "# Add ops to save and restore all the variables.\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "## Create a session\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "config.allow_soft_placement = True\n",
    "config.log_device_placement = True\n",
    "sess = tf.Session(config=config)\n",
    "print(tf.test.is_gpu_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "## Add summary writers\n",
    "merged = tf.summary.merge_all()\n",
    "train_writer = tf.summary.FileWriter(os.path.join(LOG_DIR, 'train'),\n",
    "                          sess.graph)\n",
    "test_writer = tf.summary.FileWriter(os.path.join(LOG_DIR, 'test'))\n",
    "print(tf.test.is_gpu_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "## Init variables\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init, {is_training_pl:True})\n",
    "print(tf.test.is_gpu_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "##\n",
    "ops = {'pointclouds_pl': pointclouds_pl,\n",
    "       'labels_pl': labels_pl,\n",
    "       'feature_pl': feature_pl,\n",
    "       'smpws_pl': smpws_pl,\n",
    "       'is_training_pl': is_training_pl,\n",
    "       'pred': pred,\n",
    "       'loss': loss,\n",
    "       'train_op': train_op,\n",
    "       'merged': merged,\n",
    "       'step': batch,\n",
    "       'learnrate': learning_rate}\n",
    "print(tf.test.is_gpu_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "##\n",
    "train_acc_list=[]\n",
    "test_acc_list=[]\n",
    "train_loss_list=[]\n",
    "test_F1_list=[]\n",
    "print(tf.test.is_gpu_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create a session\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "config.allow_soft_placement = True\n",
    "config.log_device_placement = True\n",
    "sess = tf.Session(config=config)\n",
    "\n",
    "# Add summary writers\n",
    "merged = tf.summary.merge_all()\n",
    "train_writer = tf.summary.FileWriter(os.path.join(LOG_DIR, 'train'),\n",
    "                          sess.graph)\n",
    "test_writer = tf.summary.FileWriter(os.path.join(LOG_DIR, 'test'))\n",
    "\n",
    "# Init variables\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init, {is_training_pl:True})\n",
    "\n",
    "ops = {'pointclouds_pl': pointclouds_pl,\n",
    "       'labels_pl': labels_pl,\n",
    "       'feature_pl': feature_pl,\n",
    "       'smpws_pl': smpws_pl,\n",
    "       'is_training_pl': is_training_pl,\n",
    "       'pred': pred,\n",
    "       'loss': loss,\n",
    "       'train_op': train_op,\n",
    "       'merged': merged,\n",
    "       'step': batch,\n",
    "       'learnrate': learning_rate}\n",
    "\n",
    "train_acc_list=[]\n",
    "test_acc_list=[]\n",
    "train_loss_list=[]\n",
    "test_F1_list=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_xyz,train_label,train_weights,train_feats = [],[],[],[]\n",
    "num_train_samples = 1000\n",
    "for i in range(num_train_samples):\n",
    "    train_point_set, train_semantic_seg, train_sample_weight, train_feature_set = get_batch('train',0,\n",
    "                                                                                            npoints=NUM_POINT)\n",
    "    train_xyz.append(train_point_set)\n",
    "    train_label.append(train_semantic_seg)\n",
    "    train_weights.append(train_sample_weight)\n",
    "    train_feats.append(train_feature_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**** EPOCH 000 ****\n",
      "----\n",
      "Current batch/total batch num: 0/166\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-25ccd1821ea5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_writer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mtest_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_F1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_one_epoch_whole_scene\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_writer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-9467e9cba974>\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(sess, ops, train_writer)\u001b[0m\n\u001b[1;32m     34\u001b[0m         summary, step, _, loss_val, pred_val, lr_val = sess.run([ops['merged'], ops['step'], ops['train_op'], \n\u001b[1;32m     35\u001b[0m                                                                  ops['loss'], ops['pred'], ops['learnrate']],\n\u001b[0;32m---> 36\u001b[0;31m                                          feed_dict=feed_dict)\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0;31m#summary, step, _, loss_val, pred_val, lr_val = sess.run([ops['merged'], ops['step'], ops['train_op'],\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;31m#                                                         ops['loss'], ops['pred'], ops['learnrate']],\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1087\u001b[0m             \u001b[0mfeed_handles\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubfeed_val\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1088\u001b[0m           \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1089\u001b[0;31m             \u001b[0mnp_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubfeed_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubfeed_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1090\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1091\u001b[0m           if (not is_tensor_handle_feed and\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    490\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m     \"\"\"\n\u001b[0;32m--> 492\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "best_F1 = -1\n",
    "\n",
    "for epoch in range(MAX_EPOCH):\n",
    "    log_string('**** EPOCH %03d ****' % (epoch))\n",
    "    sys.stdout.flush()\n",
    "\n",
    "    train_loss, train_acc = train_one_epoch(sess, ops, train_writer)\n",
    "    test_acc, test_F1 = eval_one_epoch_whole_scene(sess, ops, test_writer)\n",
    "\n",
    "    train_acc_list.append(train_acc)\n",
    "    test_acc_list.append(test_acc)\n",
    "\n",
    "    test_F1_list.append(test_F1)\n",
    "\n",
    "    \n",
    "    drawF1Plot(test_F1_list, \"F1_score\")\n",
    "    \n",
    "    drawPlot(train_acc_list,test_acc_list,\"Accuracy\")\n",
    "\n",
    "    # Save the variables to disk.\n",
    "    \n",
    "    if test_F1 > best_F1:\n",
    "        best_F1 = test_F1\n",
    "        save_path = saver.save(sess, os.path.join(LOG_DIR, \"best_model_epoch_%03d.ckpt\"%(epoch)))\n",
    "        log_string(\"Model saved in file: %s\" % save_path)\n",
    "                \n",
    "    if epoch % 10 == 0:\n",
    "        save_path = saver.save(sess, os.path.join(LOG_DIR, \"model.ckpt\"))\n",
    "        log_string(\"Model saved in file: %s\" % save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      "Overall accuracy：82.04  Average F1 score：67.59 \n",
      "75.72 85.10 89.90 92.91 76.83 95.44 60.07 34.02 75.29 \n",
      "66.00 74.98 92.73 55.80 13.04 91.75 60.12 52.91 84.54 \n",
      "70.53 79.72 91.29 69.72 22.30 93.56 60.10 41.41 79.65 \n",
      "\n",
      "learn rate: 0.000020\n",
      "eval mean loss: 0.635362\n",
      "eval accuracy: 0.820402\n",
      "eval avg class acc: 0.657640\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8204022, 0.67585766)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_one_epoch_whole_scene(sess, ops, test_writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pointclouds_pl': <tf.Tensor 'Placeholder:0' shape=(?, ?, 3) dtype=float32>,\n",
       " 'labels_pl': <tf.Tensor 'Placeholder_1:0' shape=(?, ?) dtype=int32>,\n",
       " 'feature_pl': <tf.Tensor 'Placeholder_3:0' shape=(?, ?, 1) dtype=float32>,\n",
       " 'smpws_pl': <tf.Tensor 'Placeholder_2:0' shape=(?, ?) dtype=float32>,\n",
       " 'is_training_pl': <tf.Tensor 'Placeholder_4:0' shape=() dtype=bool>,\n",
       " 'pred': <tf.Tensor 'fc2/BiasAdd:0' shape=(?, ?, 9) dtype=float32>,\n",
       " 'loss': <tf.Tensor 'sparse_softmax_cross_entropy_loss/value:0' shape=() dtype=float32>,\n",
       " 'train_op': <tf.Operation 'Adam' type=AssignAdd>,\n",
       " 'merged': <tf.Tensor 'Merge/MergeSummary:0' shape=() dtype=string>,\n",
       " 'step': <tf.Variable 'Variable:0' shape=() dtype=int32_ref>,\n",
       " 'learnrate': <tf.Tensor 'Maximum:0' shape=() dtype=float32>}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "K.clear_session()\n",
    "save_path = saver.save(sess, os.path.join(LOG_DIR, \"best_model_cleared.ckpt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
